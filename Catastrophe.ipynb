{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Catastrophe.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "7mtIbhtJGg24",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Predcition"
      ]
    },
    {
      "metadata": {
        "id": "jNGqwANmVIM7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We first try to detect P waves which are the first waves to arrive at a seismic station and  use the real-time  time series data to make a prediction about the earthquake by calculating the time taken for other waves to arrive after P waves are detected. We assume the data to be stored in a dataset called time-series .\n"
      ]
    },
    {
      "metadata": {
        "id": "w1HAEM7MWNjB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from pandas import Series, DataFrame\n",
        "from pandas.tseries.offsets import Hour,Minute\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "ts1 = pd.read_csv(' /time series/') #Loading the time series data for the P waves\n",
        "ts2 = pd.read_csv('/time series/') #Loading the time series data for the S waves\n",
        "minute = Minute()\n",
        "ts1_utc = ts.tz_localize('UTC') #Conversion from naive to localized time\n",
        "ts2_utc = ts.tz_localizr('UTC')                 \n",
        "                 \n",
        "pd.date_range('date1' , 'date2', freq = '1m') #Getting values every minute\n",
        "\n",
        "ts = ts2 - ts1\n",
        "\n",
        "\n",
        "\n",
        "                 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4uI63uZhBZVb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Applying Random Forest to predict the time remaining before the S waves arrive:\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "X_train,X_test,y_train,y_test = train_test_split(ts.time,ts.values) #Splitting the data into training and test sets\n",
        "lreg = LinearRegression()\n",
        "forest = RandomForestRegressor(random_state=0)\n",
        "forest.fit(X_train,y_train)\n",
        "\n",
        "print(\"Accuracy on the training set: %f\" %forest.score(X_train,y_train))\n",
        "print(\"Accuracy on the test set: %f\" %forest.score(X_test,y_test))\n",
        "\n",
        "coeff = DataFrame(x_train.columns)\n",
        "\n",
        "coeff['Coefficient Estimate'] = Series(lreg.coef_)\n",
        "\n",
        "coeff\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "71cgPOv5G3mo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Relief\n"
      ]
    },
    {
      "metadata": {
        "id": "djVR3n-SKNsG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#We will collect the dataset of images of people residing within an area\n",
        "#Then train our model on it to predict who all are missing in case of an earthquake\n",
        "\n",
        "import numpy as np\n",
        "import numpy.random as rng\n",
        "import os\n",
        "from matplotlib.pyplot as plt\n",
        "import dill as pickle\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D\n",
        "from keras.models import Model, Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.optimizers import SGD,Adam\n",
        "from keras.losses import binary_crossentropy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mJjHBp29KOOt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def W_init(shape, name=None):\n",
        "  values = rng.normal(loc=0,scale=1e-2,size=shape)\n",
        "    return K.variable(values,name=name)\n",
        "def b_init(shape, name=None):\n",
        "  values=rng.normal(loc=0.5,scale=1e-2,size=shape)\n",
        "    return K.variable(values,name=name)\n",
        "\n",
        "input_shape = (value,value,value)\n",
        "left_input = Input(input_shape)\n",
        "right_input = Input(input_shape)\n",
        "#build convnet to use in each siamese 'leg'\n",
        "convnet = Sequential()\n",
        "convnet.add(Conv2D(64,(10,10),activation='relu',input_shape=input_shape,\n",
        "                   kernel_initializer=W_init,kernel_regularizer=l2(2e-4)))\n",
        "convnet.add(MaxPooling2D())\n",
        "convnet.add(Conv2D(128,(7,7),activation='relu',\n",
        "                   kernel_regularizer=l2(2e-4),kernel_initializer=W_init,bias_initializer=b_init))\n",
        "convnet.add(MaxPooling2D())\n",
        "convnet.add(Conv2D(128,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
        "convnet.add(MaxPooling2D())\n",
        "convnet.add(Conv2D(256,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))\n",
        "convnet.add(Flatten())\n",
        "convnet.add(Dense(4096,activation=\"sigmoid\",kernel_regularizer=l2(1e-3),kernel_initializer=W_init,bias_initializer=b_init))\n",
        "#encode each of the two inputs into a vector with the convnet\n",
        "encoded_l = convnet(left_input)\n",
        "encoded_r = convnet(right_input)\n",
        "#merge two encoded inputs with the l1 distance between them\n",
        "L1_distance = lambda x: K.abs(x[0]-x[1])\n",
        "both = merge([encoded_l,encoded_r], mode = L1_distance, output_shape=lambda x: x[0])\n",
        "prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(both)\n",
        "siamese_net = Model(input=[left_input,right_input],output=prediction)\n",
        "#optimizer = SGD(0.0004,momentum=0.6,nesterov=True,decay=0.0003)\n",
        "\n",
        "optimizer = Adam(0.00006)\n",
        "siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
        "\n",
        "siamese_net.count_params()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xZdKLBxgGrZi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "Siamese_Loader:\n",
        "    \"\"\"For loading batches and testing tasks to a siamese net\"\"\"\n",
        "    def __init__(self,Xtrain,Xval):\n",
        "        self.Xval = Xval\n",
        "        self.Xtrain = Xtrain\n",
        "        self.n_classes,self.n_examples,self.w,self.h = Xtrain.shape\n",
        "        self.n_val,self.n_ex_val,_,_ = Xval.shape\n",
        "\n",
        "    def get_batch(self,n):\n",
        "        \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
        "        categories = rng.choice(self.n_classes,size=(n,),replace=False)\n",
        "        pairs=[np.zeros((n, self.h, self.w,1)) for i in range(2)]\n",
        "        targets=np.zeros((n,))\n",
        "        targets[n//2:] = 1\n",
        "        for i in range(n):\n",
        "            category = categories[i]\n",
        "            idx_1 = rng.randint(0,self.n_examples)\n",
        "            pairs[0][i,:,:,:] = self.Xtrain[category,idx_1].reshape(self.w,self.h,1)\n",
        "            idx_2 = rng.randint(0,self.n_examples)\n",
        "            #pick images of same class for 1st half, different for 2nd\n",
        "            category_2 = category if i >= n//2 else (category + rng.randint(1,self.n_classes)) % self.n_classes\n",
        "            pairs[1][i,:,:,:] = self.Xtrain[category_2,idx_2].reshape(self.w,self.h,1)\n",
        "        return pairs, targets\n",
        "\n",
        "    def make_oneshot_task(self,N):\n",
        "        \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\n",
        "        categories = rng.choice(self.n_val,size=(N,),replace=False)\n",
        "        indices = rng.randint(0,self.n_ex_val,size=(N,))\n",
        "        true_category = categories[0]\n",
        "        ex1, ex2 = rng.choice(self.n_examples,replace=False,size=(2,))\n",
        "        test_image = np.asarray([self.Xval[true_category,ex1,:,:]]*N).reshape(N,self.w,self.h,1)\n",
        "        support_set = self.Xval[categories,indices,:,:]\n",
        "        support_set[0,:,:] = self.Xval[true_category,ex2]\n",
        "        support_set = support_set.reshape(N,self.w,self.h,1)\n",
        "        pairs = [test_image,support_set]\n",
        "        targets = np.zeros((N,))\n",
        "        targets[0] = 1\n",
        "        return pairs, targets\n",
        "\n",
        "    def test_oneshot(self,model,N,k,verbose=0):\n",
        "        \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
        "        pass\n",
        "        n_correct = 0\n",
        "        if verbose:\n",
        "            print(\"Evaluating model on {} unique {} way one-shot learning tasks ...\".format(k,N))\n",
        "        for i in range(k):\n",
        "            inputs, targets = self.make_oneshot_task(N)\n",
        "            probs = model.predict(inputs)\n",
        "            if np.argmax(probs) == 0:\n",
        "                n_correct+=1\n",
        "        percent_correct = (100.0*n_correct / k)\n",
        "        if verbose:\n",
        "            print(\"Got an average of {}% {} way one-shot learning accuracy\".format(percent_correct,N))\n",
        "        return percent_correct\n",
        "\n",
        "evaluate_every = 7000\n",
        "loss_every=300\n",
        "batch_size = 32\n",
        "N_way = 20\n",
        "n_val = 550\n",
        "siamese_net.load_weights(\"PATH\")\n",
        "best = 76.0\n",
        "for i in range(900000):\n",
        "    (inputs,targets)=loader.get_batch(batch_size)\n",
        "    loss=siamese_net.train_on_batch(inputs,targets)\n",
        "    if i % evaluate_every == 0:\n",
        "        val_acc = loader.test_oneshot(siamese_net,N_way,n_val,verbose=True)\n",
        "        if val_acc >= best:\n",
        "            print(\"saving\")\n",
        "            siamese_net.save('PATH')\n",
        "            best=val_acc\n",
        "\n",
        "    if i % loss_every == 0:\n",
        "        print(\"iteration {}, training loss: {:.2f},\".format(i,loss))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8sgF4gohK2Op",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Analysis\n"
      ]
    },
    {
      "metadata": {
        "id": "-E_2ZL0IMs-h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will be monitoring the real time seismic waves being emitted and use Neural Network to calculate the magnitude of earthquake"
      ]
    },
    {
      "metadata": {
        "id": "88quaPysReRm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#We train our model using 'Earthquake-Normalized' dataset.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rlxv9xwAR8dd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Earthquake_normalized.csv')\n",
        "col1 = df['DateTime','Latitude','Longitude','Depth']\n",
        "col2 = df['Magnitude']\n",
        "\n",
        "x = col1.as_matrix()\n",
        "y = col2.as_matrix()\n",
        "\n",
        "#Reshape\n",
        "x_features = 4 #Number of input features\n",
        "y_features = 1 #Number of input features\n",
        "samples = 23000 # Number of samples\n",
        "\n",
        "Inputx1_reshape = np.resize(x1,(samples,x_features))\n",
        "Inputy1_reshape = np.resize(y1,(samples,y_features))\n",
        "\n",
        "#Training data\n",
        "batch_size = 20000\n",
        "Inputx1_train = Inputx1_reshape[0:batch_size,:]\n",
        "Inputy1_train = Inputy1_reshape[0:batch_size,:]\n",
        "#Validation data\n",
        "val_size = 2500\n",
        "Inputx_1val = Inputx1_reshape[batch_size:batch_size+val_size,:]\n",
        "Inputy_1val = Inputy1_reshape[batch_size:batch_size+val_size,:]\n",
        "#print(Inputx_1val)\n",
        "\n",
        "\n",
        "#Hyper parameters:\n",
        "learning_rate = 0.0005\n",
        "training_iterations = 100000\n",
        "\n",
        "#Variables:\n",
        "x = tf.placeholder((tf.float32,shape=(None,x_features))\n",
        "y = tf.placeholder(tf.float32)\n",
        "                   \n",
        "#Neurons\n",
        "#L1 = Number of neurons in 1st layer\n",
        "#L2 = Number of neurons in 2nd layer\n",
        "#L3 = Number of neurons in 3rd layer\n",
        "#Layer1 weights\n",
        "W_fc1 = tf.Variable(tf.random_uniform([Xfeatures,L1])) # [input_features,Number of neurons]) \n",
        "b_fc1 = tf.Variable(tf.constant(0.1,shape=[L1]))\n",
        "#Layer2 weights\n",
        "#W_fc2 = tf.Variable(tf.random_uniform([L1,L2])) # [Number of neurons in preceding layer,Number of neurons]) \n",
        "#b_fc2 = tf.Variable(tf.constant(0.1,shape=[L2]))\n",
        "#Layer3 weights\n",
        "#W_fc3 = tf.Variable(tf.random_uniform([L1,L3])) # [Number of neurons in preceding layer,Number of neurons]) \n",
        "#b_fc3 = tf.Variable(tf.constant(0.1,shape=[L3]))\n",
        "#Output layer weights\n",
        "W_fO= tf.Variable(tf.random_uniform([L1,Yfeatures])) #  [Number of neurons in preceding layer,output_features]) \n",
        "b_fO = tf.Variable(tf.constant(0.1,shape=[Yfeatures]))    \n",
        "                   \n",
        "#Layer 1\n",
        "matmul_fc1=tf.matmul(X, W_fc1) + b_fc1\n",
        "h_fc1 = tf.nn.relu(matmul_fc1)   #ReLU activation\n",
        "#Layer 2\n",
        "#matmul_fc2=tf.matmul(h_fc1, W_fc2) + b_fc2\n",
        "#h_fc2 = tf.nn.relu(matmul_fc2)   #ReLU activation\n",
        "#Layer 3\n",
        "#matmul_fc3=tf.matmul(h_fc1, W_fc3) + b_fc3\n",
        "#h_fc3 = tf.nn.relu(matmul_fc3)   #ReLU activation\n",
        "#Output layer\n",
        "matmul_fc4=tf.matmul(h_fc1, W_fO) + b_fO\n",
        "output_layer = matmul_fc4  #linear activation     \n",
        "                   \n",
        "#Loss function\n",
        "mean_square =  tf.reduce_mean(tf.square(y_output_layer))\n",
        "train_step = tf.train.AdamOptimizer(learning_rate).minimize(mean_square)\n",
        "\n",
        "#Operation to save variables\n",
        "saver = tf.train.Saver()                   \n",
        "        \n",
        "#Initialization and session:\n",
        "init = tf.initialize_all_variables()\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    print(\"Training loss:\",sess.run([mean_square],feed_dict={X:InputX1train,Y:InputY1train}))\n",
        "    for i in range(training_iterations):\n",
        "        sess.run([train_step],feed_dict={X:InputX1train,Y:InputY1train})\n",
        "        if i%display_iterations ==0:\n",
        "            print(\"Training loss is:\",sess.run([mean_square],feed_dict={X:InputX1train,Y:InputY1train}),\"at itertion:\",i)\n",
        "            print(\"Validation loss is:\",sess.run([mean_square],feed_dict={X:InputX1v,Y:InputY1v}),\"at itertion:\",i)\n",
        "    # Save the variables to disk.\n",
        "    save_path = saver.save(sess, \"/tmp/earthquake_model.ckpt\")\n",
        "    print(\"Model saved in file: %s\" % save_path)\n",
        "\n",
        "    print(\"Final training loss:\",sess.run([mean_square],feed_dict={X:InputX1train,Y:InputY1train}))\n",
        "    print(\"Final validation loss:\",sess.run([mean_square],feed_dict={X:InputX1v,Y:InputY1v}))                   "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}